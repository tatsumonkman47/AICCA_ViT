{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SmL35jmt4xX",
    "outputId": "3f3a5f90-0949-4d66-bedd-caa8ca8e8966",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdmonkman/anaconda3/envs/please_work/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/tdmonkman/anaconda3/envs/please_work/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load Modules\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "# Pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n",
    "# cuda setup, set seed for reproducability \n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(41)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# YOU NEED THIS TO LOAD PyTorch Lightning I DON\"T KNOW WHY\n",
    "\n",
    "from jupyter_client.manager import KernelManager\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Set torch dtype to float64\n",
    "torch.set_default_dtype(torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlNSuDdiVWGA"
   },
   "source": [
    "# Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pd0D50MjhLqn"
   },
   "outputs": [],
   "source": [
    "# Modules for loading data, set data pathimport urllib\n",
    "DATASET_PATH = \"cloudimages1M\"\n",
    "CHECKPOINT_PATH = os.getcwd()\n",
    "DRIVE_PATH = \".\"\n",
    "\n",
    "# Import custom npy loader\n",
    "import sys\n",
    "sys.path.append(f\"{DRIVE_PATH}/\")\n",
    "from import_npy import npy_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXxatV_9i2pN"
   },
   "source": [
    "# Look at example pic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "NEHGcNYYAWF6",
    "outputId": "eaad199c-5a0c-4e06-c1c1-825d83ea6713"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cloudimages1M/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m img_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDRIVE_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgroup\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[img_index]\n\u001b[1;32m      5\u001b[0m img_example \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDRIVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cloudimages1M/1'"
     ]
    }
   ],
   "source": [
    "img_index = 3\n",
    "group = 1\n",
    "\n",
    "file = os.listdir(f\"{DRIVE_PATH}/{DATASET_PATH}/{group}\")[img_index]\n",
    "img_example = np.load(f\"{DRIVE_PATH}/{DATASET_PATH}/{group}/{file}\",allow_pickle=True)\n",
    "\n",
    "fig, axs = plt.subplots(2,3,figsize=(20,10))\n",
    "\n",
    "axs[0,0].pcolor(img_example[:,:,0],cmap=\"plasma\")\n",
    "axs[0,1].pcolor(img_example[:,:,1],cmap=\"plasma\")\n",
    "axs[0,2].pcolor(img_example[:,:,2],cmap=\"plasma\")\n",
    "\n",
    "axs[1,0].pcolor(img_example[:,:,3],cmap=\"plasma\")\n",
    "axs[1,1].pcolor(img_example[:,:,4],cmap=\"plasma\")\n",
    "axs[1,2].pcolor(img_example[:,:,5],cmap=\"plasma\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEYeL1m0mNRE"
   },
   "source": [
    "# Define the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7SAB4uWAZqC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "# Here we use our custom function imported from the drive directory (import_npy.py)\n",
    "AICCA_data = datasets.DatasetFolder(root=f\"{DRIVE_PATH}/{DATASET_PATH}\",\n",
    "                                    loader=npy_loader,\n",
    "                                    extensions=['.npy'])\n",
    "    \n",
    "\n",
    "print(AICCA_data)\n",
    "print(\"classes\")\n",
    "print(AICCA_data.classes)\n",
    "print(\"class_dict\")\n",
    "print(AICCA_data.class_to_idx)\n",
    "\n",
    "# Data loader works her\n",
    "# Split into train, validation, and test data\n",
    "train_length = int(0.7*len(AICCA_data))\n",
    "validation_length = int(0.2*len(AICCA_data))\n",
    "test_length = len(AICCA_data) - train_length - validation_length\n",
    "\n",
    "# Split\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(AICCA_data, (train_length, validation_length, test_length))\n",
    "\n",
    "# Verify size of datasets\n",
    "print(f\"size train_dataset: {len(train_dataset)}\")\n",
    "print(f\"size validation_dataset: {len(validation_dataset)}\")\n",
    "print(f\"size test_dataset: {len(test_dataset)}\")\n",
    "\n",
    "# \n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False, pin_memory=True,num_workers=1)\n",
    "val_loader = data.DataLoader(validation_dataset, batch_size=128, shuffle=False, drop_last=False, pin_memory=True,num_workers=1)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False, pin_memory=True,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpKkqHZIAbSY"
   },
   "source": [
    "# Try randomly sampling images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0][0], train_dataset[0][1]\n",
    "#print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")\n",
    "\n",
    "NUM_IMAGES = 5\n",
    "idx = 3\n",
    "\n",
    "cloud_images = torch.stack([test_dataset[idx][0] for idx in np.random.randint(0,high=test_length,size=NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(torch.permute(cloud_images[:,:,:,[0,1,2]],(0,3,1,2)), nrow=NUM_IMAGES, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1,2,0)\n",
    "\n",
    "plt.figure(figsize=(10*NUM_IMAGES,8))\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "#plt.savefig(\"eval_figures/example_patches0.png\",transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*128/(8*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-JcpX-9jCtV"
   },
   "source": [
    "# Transform images to patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zik008W1jBpR"
   },
   "outputs": [],
   "source": [
    "# Function for preprocessing the images into patches\n",
    "def imgs_to_patches(imgs, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ------\n",
    "    imgs: torch.Tensor containing the images of shape (Num Images, Channels, Height, Width)\n",
    "    patch_size: \n",
    "    flatten_channels: False\n",
    "    \n",
    "    Outputs\n",
    "    -------\n",
    "    imgs: \n",
    "    \"\"\"\n",
    "    # Reshape the image tensor to shape (image, channel, height, width) \n",
    "    imgs = imgs.permute(0,3,1,2)\n",
    "    B, C, H, W = imgs.shape\n",
    "    # Divide images into patches\n",
    "    imgs = imgs.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    # Reshape images to shape (image, patch, channel, height, width)\n",
    "    imgs = imgs.permute(0,2,4,1,3,5)\n",
    "    imgs = imgs.flatten(1,2)\n",
    "    # You can flatten the patches into a \"feature vector\" if you wouldlike\n",
    "    if flatten_channels:\n",
    "        imgs = imgs.flatten(2,4)\n",
    "    return imgs\n",
    "    \n",
    "    \n",
    "cloud_images = torch.stack([test_dataset[idx][0][:,:,:3] for idx in np.random.randint(0,high=test_length,size=NUM_IMAGES)], dim=0)\n",
    "img_patches = imgs_to_patches(cloud_images, 16)\n",
    "\n",
    "# Plot examples\n",
    "fig, ax = plt.subplots(cloud_images.shape[0], 1, figsize=(20,4))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(cloud_images.shape[0]):\n",
    "  img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
    "  img_grid = img_grid.permute(1,2,0)\n",
    "  ax[i].imshow(img_grid)\n",
    "  ax[i].axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(128/4)\n",
    "print(128/8)\n",
    "print(128/16)\n",
    "print(128/32)\n",
    "print(128/64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi4x3fUAjIDi"
   },
   "source": [
    "# Attention Block Prototype\n",
    "see https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2yBBLxvMjGEi"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        embed_dim: Dimensionality of input and attention feature vectors\n",
    "        hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
    "                    (usually 2 to 4x larger than embed_dim)\n",
    "        num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "        dropout: Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__() # super() is a function that allows you to initialize \n",
    "                           # attributes from the parent class\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        # nn.LayerNorm applies layer normalization over a mini-batch of inputs as\n",
    "        # described in the AttentionBlock paper. Uses the expectation value and \n",
    "        # mean to calculate them over the last D-dimensions where D is the \n",
    "        # dimension of the 'normalized_shape.'\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        # nn.MultiheadAttention applies the multiheaded attention layer from the\n",
    "        # Attention paper. 'embed_dim' and 'num_heads' are self-explanatory.\n",
    "        # During training, dropout randomly zeros some of the elements of the \n",
    "        # input tensor with probability p using samples from a Bernoulli \n",
    "        # distribution. Each channel will be zeroed out independently on\n",
    "        # every forward call. This is an effective technique for regularization\n",
    "        # and preventing the co-adaption of neurons.\n",
    "        \n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        # take the norm again\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # run the attention block\n",
    "        input_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(input_x, input_x, input_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR9Hi9cfjwtc"
   },
   "source": [
    "## Build the Vision Transformer\n",
    "Besides the transformer, we need a couple more modules:\n",
    "*    A linear projection layer that maps the input patches to a feature vector of larger size. It is implemented by a linear layer that takes each MxM patch independently as input.\n",
    "*    A classification token that is added to the input sequence. We will use the output feature vector of the classification token (CLS token) for determining the classification prediction.\n",
    "*    Learnable positional encodings that are added to the tokens before being processed by the transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. SINCE WE ARE WORKING WITH A FIXED RESOLUTION, WE CAN LEARN THE POSITIONAL ENCODINGS INSTEAD OF HAVING THE PATTERN OF SINE AND COSINE FUNCTIONS!\n",
    "*    An MLP head that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually inmplemented by a small feed-forward network or even a single linear layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9AgozcWUjxHx"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "        hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                     within the Transformer\n",
    "        num_channels: Number of channels of the input (3 for RGB)\n",
    "        num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "        num_layers: Number of layers to use in the Transformer\n",
    "        num_classes: Number of classes to predict\n",
    "        patch_size: Number of pixels that the patches have per dimension\n",
    "        num_patches: Maximum number of patches an image can have\n",
    "        dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                  on the input encoding\n",
    "        \n",
    "        Dependencies\n",
    "        ------------\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for layer in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "        \n",
    "    def forward(self, img):\n",
    "        # Preprocess input\n",
    "        img = imgs_to_patches(img, self.patch_size)\n",
    "        B, T, _ = img.shape\n",
    "        img = self.input_layer(img)\n",
    "        \n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        img = torch.cat([cls_token, img], dim=1)\n",
    "        img = img + self.pos_embedding[:,:T+1]\n",
    "        \n",
    "        # Apply Transformer\n",
    "        img = self.dropout(img)\n",
    "        img = img.transpose(0,1)\n",
    "        img = self.transformer(img)\n",
    "        \n",
    "        # Perform classification prediction\n",
    "        cls = img[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoderclass Decoder(nn.Module):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For AICCA, this parameter is 6\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z (we use 43 cases)\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 16x16 => 32x32\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sX_ZIFWuj1u7"
   },
   "outputs": [],
   "source": [
    "from lightning_fabric.utilities import optimizer\n",
    "\n",
    "class ViT_Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.forward_model = VisionTransformer(**model_kwargs)\n",
    "        self.decoder = Conv_Decoder(**model_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: patches\n",
    "        Forward function is a ViT encoder\n",
    "        Decoder function is just a simple convolutional filter\n",
    "        \"\"\"\n",
    "        y = self.forward_model(x)\n",
    "        x_hat = self.decoder(y)\n",
    "        return x_hat\n",
    "    \n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (using MSE here)\n",
    "        \"\"\"\n",
    "        x, _ = batch\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"None\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch, mode=\"train\")\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._get_reconstruction_loss(batch, mode=\"val\")\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZlRr9PoEwPvl"
   },
   "outputs": [],
   "source": [
    "def train_model(training_checkpoint_path,**kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=180,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, verbose=True, save_last=True, mode=\"max\", monitor=\"val_acc\", every_n_train_steps=100),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    print(f\"cwd: {os.getcwd()}\")\n",
    "    print(f\"CHECKPOINT_PATH: {CHECKPOINT_PATH}\")\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    \n",
    "    elif os.path.isfile(CHECKPOINT_PATH):\n",
    "        print(f\"Found checkpoint at {CHECKPOINT_PATH}\")\n",
    "        model = ViT.load_from_checkpoint(CHECKPOINT_PATH) # Load best checkpoint after training\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    else:\n",
    "        print(\"Running from scratch\")\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/midway2/tdmonkman/AICCA_proj'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8ybK5xoWc1G"
   },
   "source": [
    "# Train the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ERE10Lf-j8ct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdmonkman/anaconda3/envs/please_work/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/tdmonkman/anaconda3/envs/please_work/lib/pytho ...\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /scratch/midway2/tdmonkman/AICCA_proj\n",
      "CHECKPOINT_PATH: /scratch/midway2/tdmonkman/AICCA_proj\n",
      "Running from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, results =     train_model( '/scratch/midway2/tdmonkman/AICCA_proj', model_kwargs={\n",
    "                                    'embed_dim': 256,\n",
    "                                    'hidden_dim': 512,\n",
    "                                    'num_heads': 8,\n",
    "                                    'num_layers': 1,\n",
    "                                    'patch_size': 16,\n",
    "                                    'num_channels': 6,\n",
    "                                    'num_patches': 64,\n",
    "                                    'num_classes': 42,\n",
    "                                    'dropout': 0.2\n",
    "                                }, lr=3e-4,)\n",
    "\n",
    "\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "please_work",
   "language": "python",
   "name": "please_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
